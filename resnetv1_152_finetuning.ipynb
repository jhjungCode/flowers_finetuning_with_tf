{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## resnet19 finetuning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from matplotlib.image import imread\n",
    "\n",
    "from datasets import dataset_utils\n",
    "\n",
    "# Main slim library\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flower dataset Download and make tfrecord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from datasets import dataset_utils\n",
    "\n",
    "url = \"http://download.tensorflow.org/data/flowers.tar.gz\"\n",
    "flowers_data_dir = '/tmp/flowers'\n",
    "\n",
    "if not tf.gfile.Exists(flowers_data_dir):\n",
    "    tf.gfile.MakeDirs(flowers_data_dir)\n",
    "\n",
    "dataset_utils.download_and_uncompress_tarball(url, flowers_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download pretrained resnset152 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"http://download.tensorflow.org/models/resnet_v1_152_2016_08_28.tar.gz\"\n",
    "ckpt_file = '/tmp/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "dataset_utils.download_and_uncompress_tarball(url, ckpt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  make load_batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_batch(dataset, batch_size=32, height=224, width=224, is_training=False):\n",
    "    \"\"\"Loads a single batch of data.\n",
    "    \n",
    "    Args:\n",
    "      dataset: The dataset to load.\n",
    "      batch_size: The number of images in the batch.\n",
    "      height: The size of each image after preprocessing.\n",
    "      width: The size of each image after preprocessing.\n",
    "      is_training: Whether or not we're currently training or evaluating.\n",
    "    \n",
    "    Returns:\n",
    "      images: A Tensor of size [batch_size, height, width, 3], image samples that have been preprocessed.\n",
    "      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that can be used for visualization.\n",
    "      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.\n",
    "    \"\"\"\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset, common_queue_capacity=32,\n",
    "        common_queue_min=8)\n",
    "    image_raw, label = data_provider.get(['image', 'label'])\n",
    "    \n",
    "    # Preprocess image for usage by Inception.\n",
    "    image = inception_preprocessing.preprocess_image(image_raw, height, width, is_training=is_training)\n",
    "    \n",
    "    # Preprocess the image for display purposes.\n",
    "    image_raw = tf.expand_dims(image_raw, 0)\n",
    "    image_raw = tf.image.resize_images(image_raw, [height, width])\n",
    "    image_raw = tf.squeeze(image_raw)\n",
    "    \n",
    "    # Batch it up.\n",
    "    images, images_raw, labels = tf.train.batch(\n",
    "          [image, image_raw, label],\n",
    "          batch_size=batch_size,\n",
    "          num_threads=1,\n",
    "          capacity=2*batch_size)\n",
    "    \n",
    "    return images, images_raw, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cfd/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py:344 in __init__.: SummaryWriter.__init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:global step 1: loss = 2.1570 (4.16 sec/step)\n",
      "INFO:tensorflow:global step 2: loss = 2.1310 (2.80 sec/step)\n",
      "INFO:tensorflow:global step 3: loss = 2.0886 (2.88 sec/step)\n",
      "INFO:tensorflow:global step 4: loss = 2.1083 (2.89 sec/step)\n",
      "INFO:tensorflow:global step 5: loss = 2.1149 (2.87 sec/step)\n",
      "INFO:tensorflow:global step 6: loss = 2.0640 (2.89 sec/step)\n",
      "INFO:tensorflow:global step 7: loss = 2.1142 (2.85 sec/step)\n",
      "INFO:tensorflow:global step 8: loss = 2.0656 (2.88 sec/step)\n",
      "INFO:tensorflow:global step 9: loss = 2.0916 (2.88 sec/step)\n",
      "INFO:tensorflow:global step 10: loss = 2.0644 (2.90 sec/step)\n",
      "INFO:tensorflow:Stopping Training.\n",
      "INFO:tensorflow:Finished training! Saving model to disk.\n",
      "Finished training. Last batch loss 2.064390\n"
     ]
    }
   ],
   "source": [
    "# Note that this may take several minutes.\n",
    "import os\n",
    "\n",
    "from datasets import flowers\n",
    "from nets import resnet_v1\n",
    "from preprocessing import inception_preprocessing\n",
    "\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "image_size = resnet_v1.resnet_v1.default_image_size\n",
    "\n",
    "ckpt_dir = '/tmp/checkpoints'\n",
    "ckpt_file = os.path.join(ckpt_dir, 'resnet_v1_152.ckpt')\n",
    "\n",
    "exclude_scopes = ['resnet_v1_152/logits']\n",
    "train_scopes = ['resnet_v1_152/logits']\n",
    "\n",
    "train_dir = '/tmp/resnetv1_finetuned/'\n",
    "\n",
    "    \n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    dataset = flowers.get_split('train', flowers_data_dir)\n",
    "    images, _, labels = load_batch(dataset,  batch_size=256, height=image_size, width=image_size)\n",
    "    one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n",
    "\n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "        logits, _ = resnet_v1.resnet_v1_152(images, num_classes=dataset.num_classes, is_training=False)\n",
    "        logits = tf. squeeze(logits, [1, 2])\n",
    "\n",
    "    # Define the variables to train scopes\n",
    "    train_vars = [var for var in  tf.trainable_variables() if var.op.name.startswith(tuple(train_scopes))]\n",
    "\n",
    "    # Define the variables to initialize by excluding trainning scopes and train\n",
    "    init_vars = slim.get_variables_to_restore(exclude=exclude_scopes)\n",
    "    init_assign_op, init_feed_dict = slim.assign_from_checkpoint(ckpt_file, init_vars)\n",
    "    \n",
    "    # Initialized funciont by checkpoint files\n",
    "    def init_fn(sess):\n",
    "        sess.run(init_assign_op, init_feed_dict)\n",
    "    \n",
    "    # Specify the loss function:\n",
    "    slim.losses.softmax_cross_entropy(logits, one_hot_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "\n",
    "    # Specify the optimizer and create the train op with only last logits variables to train\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer, variables_to_train=train_vars)\n",
    "\n",
    "    # Run the training:\n",
    "    final_loss = slim.learning.train(\n",
    "       train_op,\n",
    "       logdir=train_dir,\n",
    "       init_fn=init_fn,\n",
    "       number_of_steps=10)\n",
    "    \n",
    "    print('Finished training. Last batch loss %f' % final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
